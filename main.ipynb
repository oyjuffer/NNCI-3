{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNCL ASSIGNMENT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "(50,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (50,2) and (50,) not aligned: 2 (dim 1) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 106\u001b[0m\n\u001b[1;32m    103\u001b[0m test_set \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m200\u001b[39m]\n\u001b[1;32m    105\u001b[0m network \u001b[38;5;241m=\u001b[39m Network(P\u001b[38;5;241m=\u001b[39mP, N\u001b[38;5;241m=\u001b[39mN, K\u001b[38;5;241m=\u001b[39mK, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[82], line 73\u001b[0m, in \u001b[0;36mNetwork.train\u001b[0;34m(self, t_max, train_set, test_set)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(\u001b[38;5;28mlen\u001b[39m(train_set)):\n\u001b[1;32m     72\u001b[0m     xi, tau \u001b[38;5;241m=\u001b[39m train_set[p]\n\u001b[0;32m---> 73\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforwardPass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     epoch_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculateError(sigma, tau)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochasticGradientDescent(sigma, xi, tau)\n",
      "Cell \u001b[0;32mIn[82], line 34\u001b[0m, in \u001b[0;36mNetwork.forwardPass\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Transpose x to make it a column vector\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m dot_product \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Apply hyperbolic tangent element-wise and sum for sigma. \u001b[39;00m\n\u001b[1;32m     36\u001b[0m tanh_result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(dot_product)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (50,2) and (50,) not aligned: 2 (dim 1) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    def __init__(self, P, N, K, learning_rate):\n",
    "        # P = amount of input samples\n",
    "        self.P = P\n",
    "        # N = amount of neurons in the input layer\n",
    "        self.N = N\n",
    "        # K = amount of neurons in the hidden layer\n",
    "        self.K = K\n",
    "        # eta = learning rate\n",
    "        self.eta = learning_rate\n",
    "        # W = weights form input to hidden layers\n",
    "        self.W = self.initWeights()\n",
    "        # V = weights from hidden layer to output, fixed to 1\n",
    "        self.V = np.ones((1, self.K))\n",
    "\n",
    "    def initWeights(self):\n",
    "        \n",
    "        # Generate random vectors and normalize each vector to have a norm of 1\n",
    "        weights = np.random.randn(self.N, self.K)\n",
    "        norms_squared = np.linalg.norm(weights, axis=1, keepdims=True)**2\n",
    "        normalized_weights = weights / norms_squared\n",
    "\n",
    "        return normalized_weights\n",
    "    \n",
    "    def forwardPass(self, x):\n",
    "        \"\"\"\n",
    "        Tanh activation function. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate the dot product of the first-layer weights and the input\n",
    "        dot_product = np.dot(self.W, x)\n",
    "        # Apply hyperbolic tangent element-wise and sum for sigma. \n",
    "        tanh_result = np.tanh(dot_product)\n",
    "        sigma = np.sum(self.V * tanh_result)\n",
    "\n",
    "        # Sigma is the output of the network for a given input x\n",
    "        return sigma\n",
    "\n",
    "    def stochasticGradientDescent(self, sigma, xi, tau):\n",
    "        \"\"\"\n",
    "        Stochastic gradient descent\n",
    "        \"\"\"\n",
    "        # Use the gradient with respect to its contribution to the error\n",
    "        gradient = (sigma - tau) * (1 - np.tanh(np.dot(self.W, xi))**2)\n",
    "        # Update the weights\n",
    "        self.W = self.W - self.eta * gradient * xi\n",
    "    \n",
    "    def calculateError(self, sigma, tau):\n",
    "        # Error is the quadratic difference between sigma (network output) and\n",
    "        # tau (target value)\n",
    "        return ((sigma - tau)**2)/2\n",
    "\n",
    "\n",
    "    def train(self, t_max, train_set, test_set):\n",
    "        \"\"\"\n",
    "        Train the network using stochastic gradient descent. \n",
    "        \"\"\"\n",
    "        # Select a random sample from the train_set, and perform a forward pass.\n",
    "        # Then, update the weights using the SGD algorithm.\n",
    "        # Run for t_max * P iterations.\n",
    "        # Select a random sample from the train_set, make sure that for each t,\n",
    "        # all samples are used, but in random order.\n",
    "        for epoch in range(t_max):\n",
    "            # For each epoch, keep track of the error and print the average\n",
    "            # error for the epoch.\n",
    "            epoch_error = 0\n",
    "            epoch_error_test = 0\n",
    "            for p in np.random.permutation(len(train_set)):\n",
    "                xi, tau = train_set[p]\n",
    "                sigma = self.forwardPass(xi)\n",
    "                epoch_error += self.calculateError(sigma, tau)\n",
    "                self.stochasticGradientDescent(sigma, xi, tau)\n",
    "            epoch_error /= len(train_set)\n",
    "\n",
    "            for p in np.random.permutation(len(test_set)):\n",
    "                xi, tau = test_set[p]\n",
    "                sigma = self.forwardPass(xi)\n",
    "                epoch_error_test += self.calculateError(sigma, tau)\n",
    "            epoch_error_test /= len(test_set)\n",
    "\n",
    "            print(\"Epoch: {}, Error: {} Test Error: {}\".format(epoch, epoch_error, epoch_error_test))\n",
    "\n",
    "\n",
    "# Inputs\n",
    "xi = pd.read_csv(\"data/xi.csv\", delimiter=',', header=None)\n",
    "# Labels\n",
    "tau = pd.read_csv(\"data/tau.csv\", delimiter=',', header=None)\n",
    "\n",
    "dataset = [(xi[i], tau[i]) for i in range(len(xi))]\n",
    "\n",
    "# P = amount of input samples\n",
    "P = len(xi)\n",
    "# N = input dimensionality\n",
    "N = 50\n",
    "# K = amount of neurons in the hidden layer\n",
    "K = 2\n",
    "\n",
    "# Take only the first 100 samples\n",
    "train_set = dataset[:100]\n",
    "test_set = dataset[100:200]\n",
    "\n",
    "network = Network(P=P, N=N, K=K, learning_rate=0.05)\n",
    "network.train(t_max=100, train_set=train_set, test_set=test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
