{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNCL ASSIGNMENT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m test_set \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m200\u001b[39m]\n\u001b[0;32m    106\u001b[0m network \u001b[38;5;241m=\u001b[39m Network(P\u001b[38;5;241m=\u001b[39mP, N\u001b[38;5;241m=\u001b[39mN, K\u001b[38;5;241m=\u001b[39mK, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 83\u001b[0m, in \u001b[0;36mNetwork.train\u001b[1;34m(self, t_max, train_set, test_set)\u001b[0m\n\u001b[0;32m     81\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforwardPass(xi)\n\u001b[0;32m     82\u001b[0m     epoch_error_test \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculateError(sigma, tau)\n\u001b[1;32m---> 83\u001b[0m \u001b[43mepoch_error_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Error: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Test Error: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, epoch_error, epoch_error_test))\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    def __init__(self, P, N, K, learning_rate):\n",
    "        # P = amount of input samples\n",
    "        self.P = P\n",
    "        # N = amount of neurons in the input layer\n",
    "        self.N = N\n",
    "        # K = amount of neurons in the hidden layer\n",
    "        self.K = K\n",
    "        # eta = learning rate\n",
    "        self.eta = learning_rate\n",
    "        # W = weights form input to hidden layers\n",
    "        self.W = self.initWeights()\n",
    "        # V = weights from hidden layer to output, fixed to 1\n",
    "        self.V = np.ones((1, self.K))\n",
    "\n",
    "    def initWeights(self):\n",
    "        \n",
    "        # Generate random vectors and normalize each vector to have a norm of 1\n",
    "        weights = np.random.randn(self.N, self.K)\n",
    "        norms_squared = np.linalg.norm(weights, axis=1, keepdims=True)**2\n",
    "        normalized_weights = weights / norms_squared\n",
    "\n",
    "        return normalized_weights\n",
    "    \n",
    "    def forwardPass(self, x):\n",
    "        \"\"\"\n",
    "        Tanh activation function. \n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the dot product of the first-layer weights and the input.\n",
    "        dot_product = [0, 0]\n",
    "        dot_product[0] = np.dot(self.W[:, 0], x)\n",
    "        dot_product[1] = np.dot(self.W[:, 1], x)\n",
    "        # Apply hyperbolic tangent element-wise and sum for sigma. \n",
    "        tanh_result = np.tanh(dot_product)\n",
    "        sigma = np.sum(self.V * tanh_result)\n",
    "\n",
    "        # Sigma is the output of the network for a given input x\n",
    "        return sigma\n",
    "\n",
    "    def stochasticGradientDescent(self, sigma, xi, tau):\n",
    "        \"\"\"\n",
    "        Stochastic gradient descent\n",
    "        \"\"\"\n",
    "        # Use the gradient with respect to its contribution to the error\n",
    "        gradient1 = (sigma - tau) * (1 - np.tanh(np.dot(self.W[:, 0], xi))**2)\n",
    "        gradient2 = (sigma - tau) * (1 - np.tanh(np.dot(self.W[:, 1], xi))**2)\n",
    "        # Update the weights\n",
    "        self.W[:, 0] = self.W[:, 0] - self.eta * gradient1 * xi\n",
    "        self.W[:, 1] = self.W[:, 1] - self.eta * gradient2 * xi\n",
    "    \n",
    "    def calculateError(self, sigma, tau):\n",
    "        # Error is the quadratic difference between sigma (network output) and\n",
    "        # tau (target value)\n",
    "        return ((sigma - tau)**2)/2\n",
    "\n",
    "\n",
    "    def train(self, t_max, train_set, test_set):\n",
    "        \"\"\"\n",
    "        Train the network using stochastic gradient descent. \n",
    "        \"\"\"\n",
    "        # Select a random sample from the train_set, and perform a forward pass.\n",
    "        # Then, update the weights using the SGD algorithm.\n",
    "        # Run for t_max * P iterations.\n",
    "        # Select a random sample from the train_set, make sure that for each t,\n",
    "        # all samples are used, but in random order.\n",
    "        for epoch in range(t_max):\n",
    "            # For each epoch, keep track of the error and print the average\n",
    "            # error for the epoch.\n",
    "            epoch_error = 0\n",
    "            epoch_error_test = 0\n",
    "            for p in np.random.permutation(len(train_set)):\n",
    "                xi, tau = train_set[p]\n",
    "                sigma = self.forwardPass(xi)\n",
    "                epoch_error += self.calculateError(sigma, tau)\n",
    "                self.stochasticGradientDescent(sigma, xi, tau)\n",
    "            epoch_error /= len(train_set)\n",
    "\n",
    "            for p in np.random.permutation(len(test_set)):\n",
    "                xi, tau = test_set[p]\n",
    "                sigma = self.forwardPass(xi)\n",
    "                epoch_error_test += self.calculateError(sigma, tau)\n",
    "            epoch_error_test /= len(test_set)\n",
    "\n",
    "            print(\"Epoch: {}, Error: {} Test Error: {}\".format(epoch, epoch_error, epoch_error_test))\n",
    "\n",
    "\n",
    "# Inputs\n",
    "xi = pd.read_csv(\"data/xi.csv\", delimiter=',', header=None)\n",
    "# Labels\n",
    "tau = pd.read_csv(\"data/tau.csv\", delimiter=',', header=None)\n",
    "\n",
    "dataset = [(xi[i], tau[i]) for i in range(len(xi))]\n",
    "\n",
    "# P = amount of input samples\n",
    "P = len(xi)\n",
    "# N = input dimensionality\n",
    "N = 50\n",
    "# K = amount of neurons in the hidden layer\n",
    "K = 2\n",
    "\n",
    "# Take only the first 100 samples\n",
    "train_set = dataset[:100]\n",
    "test_set = dataset[100:200]\n",
    "\n",
    "network = Network(P=P, N=N, K=K, learning_rate=0.05)\n",
    "network.train(t_max=100, train_set=train_set, test_set=test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
